### Teaching Guide â€”â€¯LangChainâ€¯JS **Embeddings**

---

#### 1â€¯|â€¯Why Embeddings Matter

* **Semantic key for search and reasoning.** Embedding models convert any piece of text into a fixedâ€‘length numericâ€¯vector that captures its meaning, enabling similarity search, clustering, and retrievalâ€‘augmented generation (RAG).â€¯([Langchain][1])
* **Foundation of every vector store.** Without embeddings you cannot populate or query a vector index; they are the first step in the RAG pipeline.â€¯([Langchain][2])

---

#### 2â€¯|â€¯Core Interface & Methods

| Method                                                  | Signature                       | Purpose                     |
| ------------------------------------------------------- | ------------------------------- | --------------------------- |
| `embedDocuments(texts: string[]) â†’Â Promise<number[][]>` | Batchâ€‘encode multiple documents | Populate a vector store     |
| `embedQuery(text: string) â†’Â Promise<number[]>`          | Encode a single search query    | Runtime similarity lookâ€‘ups |

The separate methods exist because some providers expose different endpoints or models for doc vs. query embeddings.â€¯([v03.api.js.langchain.com][3], [Langchain][2])

---

#### 3â€¯|â€¯Anatomy of an Embedding Call (OpenAI)

```ts
import { OpenAIEmbeddings } from "@langchain/openai";

const embedder = new OpenAIEmbeddings({
  model:      "text-embedding-3-small",   // providerâ€‘specific
  batchSize:  100,                        // autoâ€‘chunk large inputs
  timeout:    60_000                      // ms
});

const vecDocs  = await embedder.embedDocuments(["LangChain is ğŸ”¥", "Embeddings rock"]);
const vecQuery = await embedder.embedQuery("Why are embeddings useful?");
```

OpenAIEmbedding batches requests under the hood and respects rateâ€‘limits.â€¯([api.js.langchain.com][4])

---

#### 4â€¯|â€¯Quickâ€‘Start Pipeline (Docsâ€¯â†’â€¯Vectorsâ€¯â†’â€¯Search)

```ts
import { MemoryVectorStore }   from "langchain/vectorstores/memory";
import { TextLoader }          from "@langchain/community/document_loaders/fs/text";
import { OpenAIEmbeddings }    from "@langchain/openai";

// 1Â |Â Load raw docs
const docs = await new TextLoader("knowledge/faq.txt").load();

// 2Â |Â Create vector store
const store = await MemoryVectorStore.fromDocuments(docs, new OpenAIEmbeddings());

// 3Â |Â Semantic search
const hits = await store.similaritySearch("reset my password", 3);
```

This endâ€‘toâ€‘end snippet mirrors the docs and shows how embeddings plug directly into a store.â€¯([Langchain][5])

---

#### 5â€¯|â€¯Choosing an Embedding Provider

| Scenario                          | Provider Class                                                | Notes                                         |
| --------------------------------- | ------------------------------------------------------------- | --------------------------------------------- |
| **Cloud, bestâ€‘inâ€‘class accuracy** | `OpenAIEmbeddings`, `CohereEmbeddings`, `MistralAIEmbeddings` | Highest recall, payâ€‘perâ€‘token                 |
| **Selfâ€‘host / browser**           | `TransformerEmbeddings` (Huggingâ€¯Face)                        | Runs locally with transformers.js, no API key |
| **Enterprise GCP**                | `VertexAIEmbeddings` / *Multimodal* variant                   | Supports text, image, and hybrid queries      |

LangChain lists >10 readyâ€‘toâ€‘use integrations; swap providers by changing a single constructor line.â€¯([Langchain][5], [Langchain][6], [Langchain][7])

---

#### 6â€¯|â€¯Design Levers & Gotchas

| Lever                      | Practical Guidance                                                                                               |
| -------------------------- | ---------------------------------------------------------------------------------------------------------------- |
| **Model dimension**        | Larger vectors (1â€¯k+) often retrieve better but cost more storage and latency.                                   |
| **Batch size**             | Tune to stay under provider token limits and avoid rateâ€‘limit errors.                                            |
| **Caching**                | Wrap any embedder with `CacheBackedEmbeddings` to avoid recomputing identical texts.â€¯([api.js.langchain.com][8]) |
| **Domain relevance**       | Pick models trained on code, legal, or medical text when domain specificity matters.                             |
| **Preâ€‘norm vs. postâ€‘norm** | Most APIs output unitâ€‘norm vectors; if not, normalise before indexing.                                           |

---

#### 7â€¯|â€¯Classroom Implementation Plan

| Stage                  | Activity                                                                                  | Outcome                                |
| ---------------------- | ----------------------------------------------------------------------------------------- | -------------------------------------- |
| **Conceptâ€¯(10â€¯min)**   | Show 2â€‘D PCA plot of sentence embeddings vs. keyword TFâ€‘IDF                               | Intuitive grasp of â€œsemantic distanceâ€ |
| **Labâ€¯1â€¯(20â€¯min)**     | Use `OpenAIEmbeddings.embedQuery` on three related sentences; compute cosine similarities | Handsâ€‘on numbers                       |
| **Labâ€¯2â€¯(25â€¯min)**     | Batchâ€‘embed 100 docs, create `MemoryVectorStore`, run similarity search                   | Endâ€‘toâ€‘end RAG prep                    |
| **Labâ€¯3â€¯(20â€¯min)**     | Swap to `TransformerEmbeddings`; compare latency & recall                                 | Provider comparison                    |
| **Challengeâ€¯(15â€¯min)** | Implement `CacheBackedEmbeddings` with a simple fileâ€‘based store                          | Performance optimisation               |
| **Debriefâ€¯(10â€¯min)**   | Inspect LangSmith trace; highlight where embeddings are generated & reused                | Observability mindset                  |

---

#### 8â€¯|â€¯Key Takeaways for Students

1. **Embeddings = semantic fingerprints**â€”everything else in RAG builds on them.
2. **Two methods, two useâ€‘cases**: `embedDocuments` for indexing, `embedQuery` for lookup.
3. **Provider swap is trivial** in LangChain; choose based on cost, privacy, and domain.
4. **Cache aggressively**â€”identical text should never hit the network twice.
5. **Tune batch size, normalisation, and model dimension** before blaming retrieval quality.

[1]: https://js.langchain.com/docs/concepts/embedding_models/?utm_source=chatgpt.com "Embedding models - LangChain.js"
[2]: https://js.langchain.com/v0.1/docs/modules/data_connection/text_embedding/?utm_source=chatgpt.com "Text embedding models - LangChain.js"
[3]: https://v03.api.js.langchain.com/interfaces/_langchain_core.embeddings.EmbeddingsInterface.html?utm_source=chatgpt.com "Interface EmbeddingsInterface - LangChain.js"
[4]: https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html?utm_source=chatgpt.com "Class OpenAIEmbeddings - LangChain.js"
[5]: https://js.langchain.com/docs/integrations/text_embedding/?utm_source=chatgpt.com "Embeddings - LangChain.js"
[6]: https://js.langchain.com/docs/integrations/text_embedding/transformers/?utm_source=chatgpt.com "HuggingFace Transformers - LangChain.js"
[7]: https://js.langchain.com/v0.1/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai/?utm_source=chatgpt.com "Google Vertex AI - LangChain.js"
[8]: https://api.js.langchain.com/classes/langchain.embeddings_cache_backed.CacheBackedEmbeddings.html?utm_source=chatgpt.com "CacheBackedEmbeddings - LangChain.js"
